from __future__ import absolute_import, division, print_function

# test that compute_delta_cchalf returns required values
import pytest
import mock
from libtbx import phil
from dials.util.options import OptionParser
from dxtbx.model.experiment_list import ExperimentList
from dxtbx.model import Crystal, Experiment, Scan
from dials.algorithms.scaling.model.scaling_model_factory import KBSMFactory
from dials.array_family import flex
from dials.command_line.compute_delta_cchalf import Script as DeltaCCHalfScript
from dials.command_line.scale_and_filter import ScaleAndFilter, AnalysisResults


def generate_test_reflections(n=2):
    reflections = flex.reflection_table()
    for id_ in range(0, n):
        r = flex.reflection_table()
        r["id"] = flex.int(10, id_)
        r["xyzobs.px.value"] = flex.vec3_double([(0, 0, i + 0.5) for i in range(0, 10)])
        r.experiment_identifiers()[id_] = str(id_)
        r.set_flags(flex.bool(10, True), r.flags.integrated)
        reflections.extend(r)
    return reflections


# @pytest.fixture
def generated_params():
    """Generate a param phil scope."""
    phil_scope = phil.parse(
        """
      include scope dials.algorithms.scaling.scaling_options.phil_scope
      include scope dials.algorithms.scaling.scaling_refiner.scaling_refinery_phil_scope
  """,
        process_includes=True,
    )
    optionparser = OptionParser(phil=phil_scope, check_format=False)
    parameters, _ = optionparser.parse_args(
        args=[], quick_parse=True, show_diff_phil=False
    )
    parameters.__inject__("model", "KB")
    parameters.scaling_options.space_group = "P2"
    return parameters


def get_scaling_model():
    return KBSMFactory.create(generated_params(), [], [])


def generate_test_experiments(n=2):
    experiments = ExperimentList()
    exp_dict = {
        "__id__": "crystal",
        "real_space_a": [1.0, 0.0, 0.0],
        "real_space_b": [0.0, 1.0, 0.0],
        "real_space_c": [0.0, 0.0, 2.0],
        "space_group_hall_symbol": " C 2y",
    }
    crystal = Crystal.from_dict(exp_dict)
    scan = Scan(image_range=[1, 10], oscillation=[0.0, 1.0])
    experiments.append(
        Experiment(crystal=crystal, scan=scan, scaling_model=get_scaling_model())
    )
    experiments[0].identifier = "0"
    if n > 1:
        for i in range(n - 1):
            experiments.append(
                Experiment(
                    crystal=crystal, scan=scan, scaling_model=get_scaling_model()
                )
            )
            experiments[i + 1].identifier = str(i + 1)
    return experiments


def generated_param():
    """Generate the default scaling parameters object."""
    phil_scope = phil.parse(
        """
      include scope dials.command_line.scale_and_filter.phil_scope
  """,
        process_includes=True,
    )

    optionparser = OptionParser(phil=phil_scope, check_format=False)
    parameters, _ = optionparser.parse_args(
        args=[], quick_parse=True, show_diff_phil=False
    )
    return parameters


def mock_experiments(n=3):
    explist = []
    for i in range(n):
        exp = mock.MagicMock()
        exp.identifier = str(i)
        exp.scan.return_value = True
        exp.scan.get_image_range.return_value = (1, 10)
        explist.append(exp)
    return exp


def _mock_scaler():
    scaler = mock.Mock()
    scaler.run.return_value = None
    return scaler


def _scaling_script(*args):
    return _mock_scaler()


def _mock_results(last_cycle_results):
    mock_results = mock.Mock()
    mock_results.finish = mock.MagicMock()
    mock_results.get_last_cycle_results.return_value = last_cycle_results
    mock_results.get_cycle_results.return_value = []
    return mock_results


test_input = [
    [{"n_removed": 0}, "no_more_removed"],
    [{"n_removed": 100, "cumul_percent_removed": 50}, "max_percent_removed"],
    [
        {
            "n_removed": 10,
            "cumul_percent_removed": 1,
            "merging_stats": {"completeness": 90},
        },
        "below_completeness_limit",
    ],
    [
        {
            "n_removed": 10,
            "cumul_percent_removed": 1,
            "merging_stats": {"completeness": 98},
        },
        "max_cycles",
    ],
]


@pytest.mark.parametrize("last_cycle_results, expected_termination", test_input)
def test_scale_and_filter_termination(last_cycle_results, expected_termination):
    """Test the termination criteria based on a mock results object."""
    exp = mock_experiments(3)
    refl = []
    params = generated_param()
    params.filtering.min_completeness = 95.0

    mock_res = _mock_results(last_cycle_results)

    scale_and_filter = ScaleAndFilter(
        params, params.scale, params.delta_cc_half, _scaling_script, _scaling_script
    )
    with mock.patch.object(
        scale_and_filter, "log_cycle_results", return_value=mock_res
    ):
        results = scale_and_filter.scale_and_filter(exp, refl)
        results.finish.assert_called_with(termination_reason=expected_termination)


def test_scale_and_filter_results_logging():
    """Test ScaleAndFilter.log_cycle_results method."""
    results = AnalysisResults()
    scaling_script = mock.Mock()
    scaling_script.merging_statistics_result = "stats_results"
    scaling_script.scaled_miller_array.size.return_value = 1000

    filter_script = mock.Mock()
    filter_script.results_summary = {
        "dataset_removal": {
            "mode": "image_group",
            "image_ranges_removed": [[(6, 10), 0]],
            "experiments_fully_removed": [],
            "n_reflections_removed": 50,
        },
        "per_dataset_delta_cc_half_values": {
            "delta_cc_half_values": [-0.1, 0.1, -0.2, 0.2]
        },
    }

    def _parse_side_effect(*args):
        return args[0]

    with mock.patch.object(
        results, "_parse_merging_stats", side_effect=_parse_side_effect
    ):
        res = ScaleAndFilter.log_cycle_results(results, scaling_script, filter_script)
    # test things have been logged correctly
    cycle_results = res.get_cycle_results()
    assert len(cycle_results) == 1
    assert cycle_results[0]["cumul_percent_removed"] == 100 * 50.0 / 1000.0
    assert cycle_results[0]["n_removed"] == 50
    assert cycle_results[0]["image_ranges_removed"] == [[(6, 10), 0]]
    assert cycle_results[0]["removed_datasets"] == []
    assert cycle_results[0]["delta_cc_half_values"] == [-0.1, 0.1, -0.2, 0.2]
    assert res.get_merging_stats()[0] == "stats_results"
    assert res.initial_n_reflections == 1000

    # add another cycle of results
    with mock.patch.object(
        results, "_parse_merging_stats", side_effect=_parse_side_effect
    ):
        res = ScaleAndFilter.log_cycle_results(res, scaling_script, filter_script)
    cycle_results = res.get_cycle_results()
    assert len(cycle_results) == 2
    assert cycle_results[1]["cumul_percent_removed"] == 100 * 2 * 50.0 / 1000.0
    assert cycle_results[1]["n_removed"] == 50
    assert cycle_results[1]["image_ranges_removed"] == [[(6, 10), 0]]
    assert cycle_results[1]["removed_datasets"] == []
    assert cycle_results[0]["delta_cc_half_values"] == [-0.1, 0.1, -0.2, 0.2]
    assert res.get_merging_stats()[1] == "stats_results"
    assert res.initial_n_reflections == 1000


def test_compute_delta_cchalf_returned_results():
    """Test that delta cchalf return necessary values for scale_and_filter."""

    # First check metadata recorded upon initialisation
    params = mock.Mock()
    params.mode = "dataset"
    params.stdcutoff = 6.0
    script = DeltaCCHalfScript(params, [], [])
    assert script.results_summary["dataset_removal"]["stdcutoff"] == 6.0
    assert script.results_summary["dataset_removal"]["mode"] == "dataset"

    # Check for correct recording of
    # results_summary['per_dataset_delta_cc_half_values']['delta_cc_half_values']
    summary = {}
    delta_cc = {0: -4, 1: 2, 2: -3, 3: -5, 4: 1}
    sorted_data, sorted_ccs = DeltaCCHalfScript.sort_deltacchalf_values(
        delta_cc, summary
    )
    expected_data_order = [3, 0, 2, 4, 1]
    expected_cc_order = [-5, -4, -3, 1, 2]
    assert list(sorted_data) == expected_data_order
    assert list(sorted_ccs) == expected_cc_order
    assert (
        summary["per_dataset_delta_cc_half_values"]["delta_cc_half_values"]
        == expected_cc_order
    )

    # Check for correct recording for dataset mode
    exp = generate_test_experiments(2)
    refls = generate_test_reflections(2)
    ids_to_remove = [0]
    results_summary = {"dataset_removal": {}}
    _ = DeltaCCHalfScript.remove_datasets_below_cutoff(
        exp, refls, ids_to_remove, results_summary
    )
    assert "experiments_fully_removed" in results_summary["dataset_removal"]
    assert "n_reflections_removed" in results_summary["dataset_removal"]
    assert results_summary["dataset_removal"]["experiments_fully_removed"] == ["0"]
    assert results_summary["dataset_removal"]["n_reflections_removed"] == 10

    # Check for correct recording for image group mode.
    exp = generate_test_experiments(2)
    refls = generate_test_reflections(2)
    ids_to_remove = [0, 1]
    image_group_to_expid_and_range = {
        0: (0, (1, 5)),
        1: (0, (6, 10)),
        2: (1, (1, 5)),
        3: (1, (6, 10)),
    }
    expids_to_image_groups = {0: [0, 1], 1: [2, 3]}
    results_summary = {"dataset_removal": {}}
    _ = DeltaCCHalfScript.remove_image_ranges_below_cutoff(
        exp,
        refls,
        ids_to_remove,
        image_group_to_expid_and_range,
        expids_to_image_groups,
        results_summary,
    )
    assert "experiments_fully_removed" in results_summary["dataset_removal"]
    assert "n_reflections_removed" in results_summary["dataset_removal"]
    assert "image_ranges_removed" in results_summary["dataset_removal"]
    assert results_summary["dataset_removal"]["experiments_fully_removed"] == ["0"]
    assert results_summary["dataset_removal"]["n_reflections_removed"] == 10
    assert results_summary["dataset_removal"]["image_ranges_removed"] == [
        [(6, 10), 0],
        [(1, 5), 0],
    ]
